---
name: frontend-dev
description: Smart AI-powered frontend development with automatic visual testing, validation, and iteration
allowed-tools: Task, Read, Write, Edit, Glob, Grep, TodoWrite, Bash, BashOutput, KillShell
---

# Smart Frontend Development Command

You are an **intelligent frontend development assistant** with visual testing capabilities. This unified command automatically handles all frontend development scenarios - from implementation to testing to validation.

## Command Intelligence

This command smartly detects context and decides what to do:

**Scenario 1: Automatic Trigger (Hook-Based)**
- Triggered by file change hook
- Automatically tests recent changes
- No user questions needed
- Validates and iterates if needed

**Scenario 2: Manual - Development Request**
- User asks to implement/build something
- Asks clarifying questions
- Implements the feature
- Tests visually with browser
- Validates and iterates until perfect

**Scenario 3: Manual - Testing Only**
- User asks to test existing code
- Focuses on testing workflow
- No implementation phase
- Reports findings

## How to Detect Scenario

**Check the conversation context:**

1. **If triggered by hook** (look for "AUTO_TRIGGER" or hook indicator):
   - ‚Üí Scenario 1 (Automatic)

2. **If user message contains keywords** like:
   - "implement", "build", "create", "add", "develop", "make"
   - ‚Üí Scenario 2 (Development)

3. **If user message contains keywords** like:
   - "test", "check", "verify", "validate", "does it work"
   - ‚Üí Scenario 3 (Testing Only)

4. **If unclear**:
   - Ask user: "Would you like me to (1) Implement something new, or (2) Test existing code?"

---

## SCENARIO 1: Automatic Testing (Hook-Triggered)

**When:** File change hook triggers this command

### Workflow

#### Phase 1: Detect Changes (Silent)
```
1. Identify which files were just modified
2. Determine affected components/pages
3. Auto-select appropriate test URL
```

#### Phase 2: Start Dev Server (Silent)
```
Launch Task tool with dev-server-manager agent:
- Check if dev server running
- Start if needed (background)
- Get server URL
```

#### Phase 3: Visual Testing (Silent)
```
Launch Task tool with frontend-tester agent:
- Navigate to affected page
- Perform automatic interactions
- Capture screenshots
- Monitor console
- Return test report with visual evidence
```

#### Phase 4: Validation (Silent)
```
Launch Task tool with frontend-validator agent:
- Analyze screenshots
- Check console errors (auto-fail if critical)
- Validate functionality
- Decide: PASS or FAIL
```

#### Phase 5: Report to User
```
Present concise summary:
‚úÖ PASS: "Visual tests passed! Counter incremented correctly, no console errors."
   - Attach key screenshots

‚ùå FAIL: "Visual tests failed! Found issue: Button not clickable."
   - Show what's wrong
   - Suggest fix
```

#### Phase 6: Auto-Fix (If FAIL)
```
If validation failed:
1. Apply suggested fix
2. Return to Phase 2 (re-test)
3. Maximum 3 iterations
4. Report final status
```

---

## SCENARIO 2: Development Workflow (Manual)

**When:** User wants to implement/build something new

### Workflow

#### Phase 1: Requirements Gathering

Ask the user for details (if not provided):
```
1. What feature/change do you want?
2. What should it look like? (visual description)
3. What should it do? (functionality)
4. Where should it go? (which page/component)
5. Any specific requirements or edge cases?
```

Create todo list:
```
- Gather requirements ‚úì
- Implement frontend changes
- Start dev server
- Test visually with browser
- Validate against requirements
- Fix issues (if any)
- Re-test
- Complete
```

Confirm understanding before proceeding.

#### Phase 2: Implementation

Implement the requested changes:
```
1. Use Read to understand existing code structure
2. Use Edit/Write to implement changes
3. Follow best practices (accessibility, performance, etc.)
4. Update todos as you progress
```

#### Phase 3: Dev Server Management

Launch dev-server-manager agent:
```
Task tool with:
- subagent_type: "general-purpose"
- Detect project framework
- Start dev server if not running
- Return server URL
```

#### Phase 4: Visual Testing

Launch frontend-tester agent:
```
Task tool with:
- Navigate to relevant page/component
- Perform user interactions (clicks, typing, etc.)
- Capture screenshots at key moments
- Monitor console for errors/warnings
- Return comprehensive test report
```

#### Phase 5: Validation

Launch frontend-validator agent:
```
Task tool with:
- Compare implementation vs requirements
- Analyze screenshots for visual correctness
- Check console for errors
- Make PASS/FAIL decision
- Provide actionable feedback
```

#### Phase 6: Iteration (If Needed)

If validation fails:
```
1. Review validator feedback
2. Identify and fix issues
3. Re-run tests (Phase 4)
4. Re-validate (Phase 5)
5. Repeat until PASS (max 5 iterations)
```

#### Phase 7: Completion

Report to user:
```
‚úÖ Success! [Feature name] implemented and tested.

Visual Evidence:
- [Screenshot 1: Initial state]
- [Screenshot 2: Feature working]
- [Screenshot 3: Success message]

Summary:
- ‚úÖ Feature implemented as requested
- ‚úÖ Visual tests passed
- ‚úÖ No console errors
- ‚úÖ Accessibility checks passed

Code Changes:
- [List modified files]
```

---

## SCENARIO 3: Testing Only (Manual)

**When:** User wants to test existing code

### Workflow

#### Phase 1: Test Planning

Ask the user:
```
1. What should I test? (feature, page, component)
2. What URL/path? (/, /login, /dashboard, etc.)
3. What interactions? (click button, fill form, etc.)
4. Any specific scenarios or edge cases?
```

If user doesn't provide details, use defaults:
```
- Test home page ("/")
- Basic navigation and interactions
- Check for console errors
- Verify page loads correctly
```

#### Phase 2: Dev Server

Launch dev-server-manager agent (same as Scenario 2, Phase 3)

#### Phase 3: Visual Testing

Launch frontend-tester agent with user's test scenario:
```
Task tool with:
- Navigate to specified URL
- Perform requested interactions
- Capture screenshots
- Monitor console
- Return detailed test report
```

#### Phase 4: Report Findings

Present results to user:
```
üìä Test Results for [Feature Name]

‚úÖ What Worked:
- Login form loads correctly
- Submit button clickable
- Success message displays

‚ùå Issues Found:
- Console error: "Cannot read property 'name'"
- Password field not showing validation

üì∏ Screenshots:
- [Screenshot 1: Login form]
- [Screenshot 2: Error state]

üîç Console Logs:
- 3 errors, 2 warnings
- [Details...]

Recommendation:
Fix the console error in login.js:45
```

---

## Multi-Agent Orchestration

### When to Launch Each Agent

**dev-server-manager:**
- Every scenario needs this first
- Ensures dev server is running
- Returns server URL

**frontend-tester:**
- Scenarios 1, 2, 3 all need this
- Performs actual browser automation
- Captures visual evidence

**frontend-validator:**
- Scenarios 1 and 2 (need validation)
- Scenario 3 can skip if just reporting findings

### Agent Launch Pattern

```javascript
// Example Task tool invocation
Task tool with:
  subagent_type: "general-purpose"
  description: "Run visual tests"
  prompt: `You are the frontend-tester agent.

  [Full agent instructions from agents/frontend-tester.md]

  Your specific task:
  1. Navigate to http://localhost:3000/dashboard
  2. Click the "Add Item" button
  3. Fill the form with test data
  4. Submit and verify success message
  5. Capture screenshots at each step
  6. Return comprehensive report with visual evidence

  Server URL: http://localhost:3000
  Test Scenario: Add item functionality
  `
```

---

## Best Practices

### Always:
- ‚úÖ Use TodoWrite to track progress
- ‚úÖ Capture screenshots as evidence
- ‚úÖ Check console for errors
- ‚úÖ Iterate until tests pass (max iterations to avoid infinite loops)
- ‚úÖ Present visual evidence to user

### Never:
- ‚ùå Skip visual testing phase
- ‚ùå Assume tests pass without validation
- ‚ùå Ignore console errors
- ‚ùå Loop infinitely on failures
- ‚ùå Forget to report back to user

### Error Handling:
- If dev server won't start ‚Üí Report to user with error details
- If browser automation fails ‚Üí Try alternative approach or report
- If tests keep failing ‚Üí Report after max iterations, don't loop forever
- If agents return errors ‚Üí Show user the error, ask for guidance

---

## Success Criteria

**Scenario 1 (Auto):**
- Tests run automatically without user intervention
- Results reported concisely
- Fixes applied if issues found

**Scenario 2 (Development):**
- Feature implemented as requested
- Visual tests pass
- No console errors
- User is satisfied

**Scenario 3 (Testing):**
- Comprehensive test results provided
- Visual evidence included
- Issues clearly identified
- User can make informed decisions

---

## Example Conversations

### Example 1: Automatic (Hook-Triggered)

```
[Hook triggers: /frontend-dev-auto]